{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "beRrZCGUAJYm",
        "w6K7xa23Elo4",
        "H0kj-8xxnORC",
        "GF8Ens_Soomf",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AniketGhorpade/NYC_TaxiTime_Regression/blob/main/NYC_TaxiTime_Prediction_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression (Supervised ML)\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1** - Aniket K. Ghorpade"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Available data in this project was released by New york taxi and Limousine commision. which includes pickup time, geo-coordinates, number of passengers and sevral other variables. Our main task in this project was to build a machine learning model which will predict trip duration time. It was a supervised machine learning problem and we have to predict a contineous variable that's why we have used regression based machine learning algorithms in this project.\n",
        "\n",
        "Intially we have performed data cleanig and data transformation to bring available data in desirable format. Then we have performed sevral data transformations and derived few new features so that it will be easier to do exploratory analysis. After that we have perfomrmed exploratory data analysis respectively we did univariate analysis, bia-variate analysis to check relationship between different variables followed by multi-variate analysis, so it gave us various insights which are useful to take business decisions. With all above procedure we have finished our exploratory data analysis part of this project.\n",
        "\n",
        "Then we have performed hypothesis testing to check various hypothetical statements by using various statistical methods. \n",
        "\n",
        "After all above procedure we have moved towards machine learning part of our project in which we have made our data model ready by doing various transformations such as label encoding, outlier handlings etc. Then we have selected few machine learning models which are suitable in our case as per the objective of our project. After selecting those 3-4 models we have fitted traning data and tested with our test data. Then did evaluation of those models by using various accuracy metrices and cross validation techniques. Then repeated this process again by doing hyper parameter tunning and this is we are able to find the best supervised regression machine learning model for above problem statment with tunned hyperparameters."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "baO8oe4GdIiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/AniketGhorpade/NYC_TaxiTime_Regression"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task is to build a model that predicts the total ride duration of taxi trips in New York City. Apart from that we have to do exploratory data analysis on availble data set which which will be useful to find insights from the dataset.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Data Maipulation Libraries\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "from datetime import datetime\n",
        "\n",
        "## Statistics Library\n",
        "import scipy.stats as stats\n",
        "\n",
        "## Data Visualisation Libraray\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "import seaborn as sns\n",
        "\n",
        "## Warnings \n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "## Machine Learning \n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
        "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n",
        "from sklearn.ensemble import AdaBoostRegressor,GradientBoostingRegressor,RandomForestRegressor \n",
        " \n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score,mean_squared_error"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Capstone Projects Submission/2. ML Regression - NYC Taxi Time /NYC Taxi Data.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().value_counts()"
      ],
      "metadata": {
        "id": "xkZ6w1pvnn7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No any duplicate value is there in the dataset."
      ],
      "metadata": {
        "id": "TFNE68VFqQVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, No any missing value is there in dataset."
      ],
      "metadata": {
        "id": "otoZtDmzqeQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primary dataset is one released by the NYC Taxi and Limousine Commission, which include pickup time, geo-coordinates, number of passengers, and several other variables."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Id : A unique identifier for each trip\n",
        "2.   Vendor_id : A code indicating the provider associated with trip record \n",
        "3. pickup_datetime : date and time when the meter was engaged \n",
        "4. dropoff_datetime : date and time when the meter was disengaged\n",
        "5. passenger_count : the number of passengers in the vehicle, driver entered value\n",
        "6. pickup lattitude : geographical co-ordinates \n",
        "7. dropup lattitude : geographical co-ordinates \n",
        "8. pickup longitude : geographical co-ordinates \n",
        "9. dropup longitude : geographical co-ordinates \n",
        "10. store_and_fwd_flag : weather the trip record was held in the vehicle memory,before sending to the server because vehicle did not have a connection to the server.\n",
        "11. trip_duration : duration of trip in seconds, this is dependant variable\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UxxkAEcxsLyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ID, Vendor ID : Represents various ID's of vendors and respective rows, all values in this columns are unique.This are descrete feature we are not going to consider them in analysis as well as in model building.\n",
        "2. pickup_datetime, dropup_datetime: Almost all values are unique collected from 2016-01-01 to 2016-06-30. \n",
        "3. On an avg. 1-2 passengers are trvelling in one car at a time, range of passenger's is from 1 to 9, and median is 1, most of the time only 1 passenger is travelling at a time.\n",
        "4. Pickup longitude varies in range between -121.93334197998048 to -61.33552932739258.\n",
        "5. Pickup latitude varies in range between 34.35969543457031 to 51.88108444213867.\n",
        "6. Dropoff longitude varies in range between -121.9333038330078 to -61.33552932739258.\n",
        "7. Dropoff latitude varies in range between 32.1811408996582 to 43.92102813720703.\n",
        "8. store_and_fwd_flag is Off in 99.5 % cases. \n",
        "9. Avg. duration of trip is 960 seconds. Minimum is 1 seconds and maximum is 3526282. Median value is 653. There are chances that distribution should be positively skewed."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for col in df.columns:\n",
        "  print(f'{col} : {len(df[col].unique())}')"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "df.drop(['id'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['trip_dur_min'] = round(df['trip_duration']/60,2)"
      ],
      "metadata": {
        "id": "7nrDah7yePqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calculating distance, by using lattitudes and longitudes"
      ],
      "metadata": {
        "id": "8xQ_vP3FAmgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import radians, cos, sin, asin, sqrt\n",
        "def distance(lat1, lat2, lon1, lon2):\n",
        "     \n",
        "    # The math module contains a function named\n",
        "    # radians which converts from degrees to radians.\n",
        "    lon1 = radians(lon1)\n",
        "    lon2 = radians(lon2)\n",
        "    lat1 = radians(lat1)\n",
        "    lat2 = radians(lat2)\n",
        "      \n",
        "    # Haversine formula\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
        " \n",
        "    c = 2 * asin(sqrt(a))\n",
        "    \n",
        "    # Radius of earth in kilometers. Use 3956 for miles\n",
        "    r = 6371\n",
        "      \n",
        "    # calculate the result\n",
        "    return(c * r)"
      ],
      "metadata": {
        "id": "mfAFy8nCAkif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['distance'] = df.apply(lambda row:distance(row['pickup_latitude'],row['dropoff_latitude'],row['pickup_longitude'],row['dropoff_longitude']),axis=1)"
      ],
      "metadata": {
        "id": "mLpeu4SABnI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= df[df['distance']<70]\n",
        "df= df[df['trip_dur_min']<100]"
      ],
      "metadata": {
        "id": "bPNfQRt6lhk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####1.Created column day of the week to do analysis on weekday basis.\n",
        "#####2.Created column speed to do analysis of time that drivers are taking w.r.t distance"
      ],
      "metadata": {
        "id": "r_umrQELIlbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['pickup_datetime'] = df.apply(lambda row: datetime.strptime(row['pickup_datetime'],'%Y-%m-%d %H:%M:%S'),axis=1)"
      ],
      "metadata": {
        "id": "yd7J-v5kKkRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['dropoff_datetime'] = df.apply(lambda row: datetime.strptime(row['dropoff_datetime'],'%Y-%m-%d %H:%M:%S'),axis=1)"
      ],
      "metadata": {
        "id": "coODK1rsNep0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['pickup_weekday'] = df.apply(lambda row: row['pickup_datetime'].strftime(\"%A\"),axis=1) #Monday=1"
      ],
      "metadata": {
        "id": "xjDsoRlgIkc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['pickup_hour'] = df.apply(lambda row: row['pickup_datetime'].hour,axis=1)"
      ],
      "metadata": {
        "id": "ezur-dJ3O_4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['speed'] = (df['distance'] * 60) / df['trip_dur_min'] \n",
        "df= df[df['speed']<80]"
      ],
      "metadata": {
        "id": "JZOVS-3Zt6dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "euVXtm11Gw73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dropped Id column as we are not going to consider descrete column to do data analysis.\n",
        "2. Created one column for trip duration in minutes.\n",
        "3. Calculated distance in kilometers by using pickup,dropoff latitude and longitudes.\n",
        "4. Converted data types of pickup and dropoff datetime columns into datetime format.\n",
        "5. Fetched pickup weekday, pickup hour for further analysis from pickup datetime column \n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**I) Analysis for two different vendors**"
      ],
      "metadata": {
        "id": "naps0IZMTCnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**1. Percentage share in rides**"
      ],
      "metadata": {
        "id": "dFRiKN4aTkO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vendor_per = pd.DataFrame(df.vendor_id.value_counts().reset_index())\n",
        "vendor_per.rename(columns = {'index':'Vendor ID','vendor_id':'Total Trips'}, inplace = True)\n",
        "\n",
        "## Pie Chart\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "data = vendor_per['Total Trips']\n",
        "keys = vendor_per['Vendor ID']\n",
        "  \n",
        "# declaring exploding pie\n",
        "explode = [0.01,0.01]\n",
        "# define Seaborn color palette to use\n",
        "palette_color = sns.color_palette('Dark2')\n",
        "  \n",
        "# plotting data on chart\n",
        "plt.pie(data, labels=['Vendor ID 2','Vendor ID 1'], colors=palette_color,explode=explode,autopct='%.0f%%',textprops={'fontsize': 14})\n",
        "\n",
        "\n",
        "plt.legend(fontsize=14)\n",
        "\n",
        "plt.title(\"Share With Respect To Vendor\",size=16)\n",
        "# displaying chart\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "7eMKQppcTB8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Pie chart is showing distribution of trips with respect to vendors.\n",
        "2. There is no much biasness in distribution, both the vendors had trips around 50 % as per data."
      ],
      "metadata": {
        "id": "m-zB6EPf6-uD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**2. Operation areas of vendor 1 and vender 2 (Operation Range)**"
      ],
      "metadata": {
        "id": "54fei2mgeURc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_long_p = df.groupby('vendor_id').agg({'pickup_longitude':['min','max']})\n",
        "df_lat_p = df.groupby('vendor_id').agg({'pickup_latitude':['min','max']})\n",
        "df_long_d = df.groupby('vendor_id').agg({'dropoff_longitude':['min','max']})\n",
        "df_lat_d = df.groupby('vendor_id').agg({'dropoff_latitude':['min','max']})"
      ],
      "metadata": {
        "id": "J1z2TyVCSFu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_long_p)\n",
        "print(df_lat_p)\n",
        "print(df_long_d)\n",
        "print(df_lat_d)"
      ],
      "metadata": {
        "id": "xazSylWdaG1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1) Vendor id 1 : minimum longitude -79.487900, maximum longitude -61.335529, minimum lattitude 34.359695, maximum lattitude 43.911762\n",
        "\n",
        "2) Vendor id 2 : minimum longitude -121.933342, maximum longitude -73.092278, minimum lattitude 34.359695, maximum lattitude 41.693352*"
      ],
      "metadata": {
        "id": "RSxAXzQQcXRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**3. Server connection issues with respect to vendors**"
      ],
      "metadata": {
        "id": "1YpiFxxrgSBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_flag = df.groupby('vendor_id')\n",
        "df_flag = pd.DataFrame(df_flag['store_and_fwd_flag'].value_counts())\n",
        "df_flag.rename(columns = {'store_and_fwd_flag':'store_and_fwd_flag counts'}, inplace = True)\n",
        "df_flag.reset_index(inplace=True)\n",
        "df_flag"
      ],
      "metadata": {
        "id": "egVVPf1vfDRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) As per above table vendor with Id-1 is facing server connection issues that is also only in 1.5 % cases.\n",
        "\n",
        "2) Vendor 2 dont have any problem in terms of connection with server there is not even a single incident when vendor 2 has faced any network connection issue."
      ],
      "metadata": {
        "id": "vEVkiAE5pza7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**4. Description of trip duration for Vendor 1 and Vendor 2**"
      ],
      "metadata": {
        "id": "mjEtFa-958qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('vendor_id')['trip_dur_min'].describe()"
      ],
      "metadata": {
        "id": "q4UO8_Fp5aNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Above table is showing discription of trip duration in minutes for both the vendors, it is almost equal for both the vendors.\n",
        "\n",
        "2) Avg. trip duration is 13-14 minutes per trip, other distribution in terms of various quantiles and median is also almost equal.\n",
        "\n",
        "3) We can conclude from above table that both the vendors are getting trips with almost equal duration."
      ],
      "metadata": {
        "id": "VGzng91a7zbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**5. Description of distance for Vendor 1 and Vendor 2**"
      ],
      "metadata": {
        "id": "A-gKI_Vm7mAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('vendor_id')['distance'].describe()"
      ],
      "metadata": {
        "id": "Z8P7yh4y7ux6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Above table is showing discription of distance in kilometers for both the vendors, it is almost equal for both the vendors same as there trip durations.\n",
        "\n",
        "2) Avg. trip distance is 3.5 km per trip for both the vendors, other distribution in terms of various quantiles and median is also almost equal.\n",
        "\n",
        "3) We can conclude from above table that both the vendors are getting trips of equal distances."
      ],
      "metadata": {
        "id": "liEZ0Qfc81lZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.Distribution of contineous variables, Univariate Analysis**"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**I) Distribution of passenger count**"
      ],
      "metadata": {
        "id": "eFCdBD0T-Z4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def distr(DF,x,n_bins=10):\n",
        "\n",
        "  ''' This function gives univariate distribution of contineous variable in the form of Histogram '''\n",
        "\n",
        "  legend = ['distribution']\n",
        "\n",
        "  # Creating histogram\n",
        "  fig, axs = plt.subplots(1, 1,\n",
        "                          figsize =(8,5),\n",
        "                          tight_layout = True)\n",
        "  \n",
        "  \n",
        "  # Remove axes splines\n",
        "  for s in ['top', 'bottom', 'left', 'right']:\n",
        "      axs.spines[s].set_visible(False)\n",
        "  \n",
        "  # Remove x, y ticks\n",
        "  axs.xaxis.set_ticks_position('none')\n",
        "  axs.yaxis.set_ticks_position('none')\n",
        "    \n",
        "  # Add padding between axes and labels\n",
        "  axs.xaxis.set_tick_params(pad = 5)\n",
        "  axs.yaxis.set_tick_params(pad = 10)\n",
        "  \n",
        "  # Add x, y gridlines\n",
        "  axs.grid(b = True, color ='grey',\n",
        "          linestyle ='-.', linewidth = 0.5,\n",
        "          alpha = 0.6)\n",
        "  kur = round(DF[x].kurt(),2)\n",
        "  ske = round(DF[x].skew(),2)\n",
        "  # Add Text watermark\n",
        "  fig.text(0.9, 0.75, 'Kurtosis: '+str(kur)+' & Skewness: '+str(ske),\n",
        "          fontsize = 14,\n",
        "          color ='black',\n",
        "          ha ='right',\n",
        "          va ='bottom',\n",
        "          alpha = 1.0)\n",
        "  \n",
        "  # Creating histogram\n",
        "  N, bins, patches = axs.hist(DF[x], bins = n_bins)\n",
        "  \n",
        "  # Setting color\n",
        "  fracs = ((N**(1 / 5)) / N.max())\n",
        "  norm = colors.Normalize(fracs.min(), fracs.max())\n",
        "  \n",
        "  for thisfrac, thispatch in zip(fracs, patches):\n",
        "      color = plt.cm.viridis(norm(thisfrac))\n",
        "      thispatch.set_facecolor(color)\n",
        "  \n",
        "  # Adding extra features   \n",
        "  plt.xlabel(x,size=14)\n",
        "  plt.ylabel(\"Count\",size=14)\n",
        "  plt.legend(legend)\n",
        "  plt.title('Distribution Of '+x,size=16)\n",
        "  \n",
        "  # Show plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "OGHyTzuUBnSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distr(DF=df,x='passenger_count')\n",
        "print(f'Avg. Passengers At A Time : {round(df.passenger_count.mean(),2)}')\n",
        "print(f'Max. Passengers At A Time : {round(df.passenger_count.max(),2)}')"
      ],
      "metadata": {
        "id": "Ls_i-6I0DPVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.With kurtosis 3.43 we could say that there is high peakedness in the data, mean is representing data very well kurtosis is highly positive.\n",
        "2.With skewness 2.13 it is clearly showing positively skewed distribution, most of the time 1 or 2 passengers are travelling at a time in NYC by using taxi."
      ],
      "metadata": {
        "id": "B1Ll3bZjgHQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**II) Distribution of distance travelled by customer**"
      ],
      "metadata": {
        "id": "lAaDIYaChTkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distr(DF=df,x='distance',n_bins=20)\n",
        "print(f'Avg. distance of a trip : {round(df.distance.mean(),2)} km')\n",
        "print(f'Median distance of a trip : {round(df.speed.median(),2)} km')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.With kurtosis 11.18 we could say that there is high peakedness in the data, mean is representing data very well kurtosis is highly positive, it is leptokurtic distibution.\n",
        "\n",
        "2.With skewness 2.95 it is clearly showing positively skewed distribution, most of the time 1-10 km is the distance for which customers preffer taxi in NYC."
      ],
      "metadata": {
        "id": "MpCkOeQDoy6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####**III) Distribution of time taken by driver**"
      ],
      "metadata": {
        "id": "2H53hn31qFvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distr(DF=df,x='trip_dur_min',n_bins=20)\n",
        "print(f'Avg. time of a trip : {round(df.trip_dur_min.mean(),2)} minutes')\n",
        "print(f'Median time of a trip : {round(df.trip_dur_min.median(),2)} minutes')"
      ],
      "metadata": {
        "id": "CAul2jHSqMdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.With kurtosis 6.87 we could say that there is high peakedness in the data, mean is representing data very well kurtosis is highly positive and distribution is highly leptokurtic.\n",
        "\n",
        "2.With skewness 2.12 it is clearly showing positively skewed distribution, most of the time 5 to 10 minutes rides has been taken by passengers in NYC by using taxi."
      ],
      "metadata": {
        "id": "LCQdYcI8pnYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "####**IV) Distribution of speed at which drivers are driving**"
      ],
      "metadata": {
        "id": "FCrx7K6VLXS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distr(DF=df,x='speed',n_bins=20)\n",
        "print(f'Avg. Speed : {round(df.speed.mean(),2)} km/hr')\n",
        "print(f'Max. Speed : {round(df.speed.max(),2)} km/hr')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.With kurtosis 3.23 we could say that there is high peakedness in the data, mean is representing data very well kurtosis is highly positive and distribution is highly leptokurtic.\n",
        "\n",
        "2.With skewness 1.41 it is clearly showing positively skewed distribution, most of the time drivers are driving at the speed of 14-20 km/hr."
      ],
      "metadata": {
        "id": "mtIoCYRVL2fj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3)Traffic w.r.t days of week , time of day**\n"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**1) Day Time Pickup distribution on hourly basis**"
      ],
      "metadata": {
        "id": "kjsMpK1WVsPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickup_hour_dist = pd.DataFrame(df.pickup_hour.value_counts().reset_index())\n",
        "pickup_hour_dist.rename(columns = {'index':'Pickup Hour','pickup_hour':'TotalTrips'}, inplace = True)\n",
        "pickup_hour_dist = pickup_hour_dist.sort_values(by='Pickup Hour')\n",
        "pickup_hour_dist.reset_index(inplace=True)\n",
        "pickup_hour_dist.drop('index',axis=1,inplace=True)\n",
        "pickup_hour_dist.head(4)"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plots = sns.barplot(x='Pickup Hour', y='TotalTrips', data=pickup_hour_dist)\n",
        "plots.set_xticklabels(plots.get_xticklabels(), rotation=1, ha=\"right\",size=12)\n",
        " \n",
        "# Iterating over the bars one-by-one\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.0f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=8, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        " \n",
        "# Setting the label for x-axis\n",
        "plt.xlabel(\"Pickup Hour\", size=14)\n",
        " \n",
        "# Setting the label for y-axis\n",
        "plt.ylabel(\"Total Trips\", size=14)\n",
        " \n",
        "# Setting the title for the graph\n",
        "plt.title(\"Pickup Hours Distribution\",size=16)\n",
        " \n",
        "# Finally showing the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y9nxr3HoStDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In day time there is very high traffic as compared to the night hours.\n",
        "2. In office timing entry and exit hours has so much rides to be picked.\n",
        "3. Graph is clearly showing that pickup traffic is dependant hour pickup hour time, and it is indirectly affecting trip duration also."
      ],
      "metadata": {
        "id": "qav5B1NsMjS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**2) Week day Pickup distribution**"
      ],
      "metadata": {
        "id": "GG1_LA7CWF1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pickup_weekday_dist = pd.DataFrame(df.pickup_weekday.value_counts().reset_index())\n",
        "pickup_weekday_dist.rename(columns = {'index':'Pickup Day','pickup_weekday':'TotalTrips'}, inplace = True)\n",
        "pickup_weekday_dist"
      ],
      "metadata": {
        "id": "-ivzHof2WSpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "sns.set(style=\"whitegrid\", color_codes=True)\n",
        "plots = sns.barplot(x='Pickup Day', y='TotalTrips', data=pickup_weekday_dist,color='deepskyblue')\n",
        "plots.set_xticklabels(plots.get_xticklabels(), rotation=1, ha=\"right\",size=12)\n",
        "\n",
        "\n",
        " \n",
        "# Iterating over the bars one-by-one\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.0f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=14, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        " \n",
        "# Setting the label for x-axis\n",
        "plt.xlabel(\"Pickup Day\", size=14)\n",
        " \n",
        "# Setting the label for y-axis\n",
        "plt.ylabel(\"Total Trips\", size=14)\n",
        " \n",
        "# Setting the title for the graph\n",
        "plt.title(\"Pickup Day Distribution\",size=16)\n",
        " \n",
        "# Finally showing the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AiwxpZNtXDC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. On weekend nights that is on fridays and saturday's there is 10 % more traffic as compared to other five working days of week.\n",
        "2. Almost similar pickups are there on weekdays."
      ],
      "metadata": {
        "id": "PiMvjHIZNO_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**4. Number of trips w.r.t time**"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfda = df[['pickup_datetime']]\n",
        "dfda['date'] = dfda.apply(lambda row: row['pickup_datetime'].date(),axis=1)"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfdate = pd.DataFrame(dfda['date'].value_counts().reset_index())\n",
        "dfdate.rename(columns = {'index':'Date','date':'Total Trips'}, inplace = True)\n",
        "dfdate.sort_values(by='Date',inplace=True)\n",
        "dfdate.reset_index(inplace=True)\n",
        "dfdate.drop('index',axis=1,inplace=True)\n",
        "dfdate.head(3)"
      ],
      "metadata": {
        "id": "qU_c2Y9-6GBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = dfdate.Date\n",
        "y = dfdate['Total Trips']\n",
        "\n",
        "plt.figure(figsize=(14, 4))\n",
        "\n",
        "\n",
        "plt.plot(x, y,color='red')\n",
        "plt.xlabel(\"Days\",size=14)  # add X-axis label\n",
        "plt.ylabel(\"Total Trips\",size=14)  # add Y-axis label\n",
        "plt.title(\"Trips Per Day Time Series\",size=16)  # add title\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "ZvE_pqPA8zAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "bzgOkGTy-OUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**5. Relationship between contineous variables**"
      ],
      "metadata": {
        "id": "bK12dwWz__1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**1. Relation Between Trip Duration & Distance**"
      ],
      "metadata": {
        "id": "m8raEmiuAP5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Scatter(X,Y):\n",
        "  '''This function gives scatter plot of two contineous variables'''\n",
        "\n",
        "\n",
        "  x = df[X]\n",
        "  y = df[Y]\n",
        "  colors = df[Y]\n",
        "  sizes = df[Y]\n",
        "\n",
        "  plt.figure(figsize = (20, 8))\n",
        "  plt.ticklabel_format(style = 'plain')\n",
        "  plt.scatter(x, y, c = colors, s = sizes, alpha = 0.3, cmap = 'PRGn') #viridis\n",
        "  plt.colorbar()\n",
        "  \n",
        "  plt.xlabel(X,size=14)\n",
        "  plt.ylabel(Y,size=14)\n",
        "  plt.title(X+\" vs \"+Y,size=16);"
      ],
      "metadata": {
        "id": "iaUMeIHQtv6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Scatter(Y='trip_dur_min',X='distance')"
      ],
      "metadata": {
        "id": "fscSFSgDNebP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**2. Relation Between Distance & Speed**"
      ],
      "metadata": {
        "id": "xjsswF6cN_cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Scatter(Y='speed',X='distance')"
      ],
      "metadata": {
        "id": "G2eyCDC3Ov04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**3. Relation Between Trip Duration & Speed**"
      ],
      "metadata": {
        "id": "G3znffPMP04O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Scatter(Y='speed',X='trip_dur_min')"
      ],
      "metadata": {
        "id": "a_p9d9jxQHkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "RHaT4dFBxV_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**6. Is there any relation between Server connection issues with taxi and trip duration or distance**"
      ],
      "metadata": {
        "id": "h_RXOKzR4vml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfnint = df[df['store_and_fwd_flag']=='N']\n",
        "dfint = df[df['store_and_fwd_flag']=='Y']"
      ],
      "metadata": {
        "id": "o5GsbYuxyOpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Distribution of trip_duration for trip's which had server connection issue's**"
      ],
      "metadata": {
        "id": "XUyGzX2z5qV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distr(DF=dfint,x='trip_dur_min',n_bins=20)\n",
        "print(f'Avg. time of a trip : {round(dfint.trip_dur_min.mean(),2)} minutes')\n",
        "print(f'Median time of a trip : {round(dfint.trip_dur_min.median(),2)} minutes') "
      ],
      "metadata": {
        "id": "qWvbjZyLzcOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Distribution of trip_duration for trip's which dont had server connection issue's**"
      ],
      "metadata": {
        "id": "0w_7tRqy6lbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distr(DF=dfnint,x='trip_dur_min',n_bins=20)\n",
        "print(f'Avg. time of a trip : {round(dfnint.trip_dur_min.mean(),2)} minutes')\n",
        "print(f'Median time of a trip : {round(dfnint.trip_dur_min.median(),2)} minutes') "
      ],
      "metadata": {
        "id": "6XRdZZCk6bty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**7. Correlation Heatmap**"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,10))\n",
        "colormap = plt.cm.plasma\n",
        "#sns.heatmap(df[['trip_dur_min','distance','pickup_hour','speed']].corr(),cmap = colormap,annot=True);\n",
        "sns.heatmap(df.corr(),cmap = colormap,annot=True);\n",
        "sns.set(font_scale=1.1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**8. Pair Plot**"
      ],
      "metadata": {
        "id": "g01HXKJWF64Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize = (20,10))\n",
        "# sns.set(font_scale=0.8)\n",
        "\n",
        "# sns.pairplot(df, hue ='vendor_id')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 1: Passengers which are travelling at a time is changing with change in vendor.\n",
        "\n",
        "Hypothesis 2: Weekends has more traffic as compared to week days.\n",
        "\n",
        "Hypothesis 3: Traffic hours on weekdays are different than traffic hours on weekends."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Hypothesis 1: Passengers which are travelling at a time is changing with change in vendor."
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H0 : 'passenger_count' and 'vendor_id' has no relationship.\n",
        "\n",
        "H1 : 'passenger_count' and 'vendor_id' has relationship."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As we are analysing two categorical variables, here We are going to use chi-squre test of independance."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new = pd.crosstab(df.passenger_count,df.vendor_id)\n",
        "new = pd.DataFrame(new)\n",
        "new.reset_index(inplace=True)\n",
        "new['Total'] = new[1] + new[2]\n",
        "new = new.iloc[1:7,:]\n",
        "new.set_index('passenger_count',inplace=True)\n",
        "new.loc['Total'] = new.iloc[:, :].sum()\n",
        "new"
      ],
      "metadata": {
        "id": "AiFoirZ1VSlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Calcualtion of Chisquare\n",
        "chi_square = 0\n",
        "rows = new.index.unique()\n",
        "columns = new.columns.unique()\n",
        "for i in columns:\n",
        "    for j in rows:\n",
        "        O = new[i][j]\n",
        "        E = new[i]['Total'] * new['Total'][j] / new['Total']['Total']\n",
        "        chi_square += (O-E)**2/E"
      ],
      "metadata": {
        "id": "0BJIIw_qg3lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The p-value approach\n",
        "print(\"Approach 1: The p-value approach to hypothesis testing in the decision rule\")\n",
        "p_value = 1 - stats.chi2.cdf(chi_square, (len(rows)-1)*(len(columns)-1))\n",
        "conclusion = \"Failed to reject the null hypothesis.\"\n",
        "if p_value <= alpha:\n",
        "    conclusion = \"Null Hypothesis is rejected.\"\n",
        "        \n",
        "print(\"chisquare-score is:\", chi_square, \" and p value is:\", p_value)\n",
        "print(conclusion)"
      ],
      "metadata": {
        "id": "D2vEg-V-hBHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The critical value approach\n",
        "print(\"Approach 2: The critical value approach to hypothesis testing in the decision rule\")\n",
        "critical_value = stats.chi2.ppf(1-alpha, (len(rows)-1)*(len(columns)-1))\n",
        "conclusion = \"Failed to reject the null hypothesis.\"\n",
        "if chi_square > critical_value:\n",
        "    conclusion = \"Null Hypothesis is rejected.\"\n",
        "        \n",
        "print(\"chisquare-score is:\", chi_square, \" and critical value is:\", critical_value)\n",
        "print(conclusion)"
      ],
      "metadata": {
        "id": "nSHwan3tkIl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 1 Conclusion: Passenger Count is dependant on Vendor_ID.**"
      ],
      "metadata": {
        "id": "MIQ7STDAkxVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Hypothesis 2: Weekends has more traffic as compared to week days."
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H0 : mean of count of trips on weekday == mean of mean of count of trips on weekend day \n",
        "\n",
        "H1 : mean of count of trips on weekday < mean of mean of count of trips on weekend day "
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfweekday = pd.DataFrame(df.pickup_weekday.value_counts())\n",
        "dfweekday.reset_index(inplace=True)\n",
        "dfweekday.rename(columns={'index':'weekday','pickup_weekday':'count'},inplace=True) \n",
        "\n",
        "mean_weekday=[]\n",
        "\n",
        "\n",
        "for i in dfweekday['weekday']:\n",
        "  if i in ['Saturday','Sunday']:\n",
        "    mean_weekday.append(2)\n",
        "  else:\n",
        "    mean_weekday.append(5)\n",
        "\n",
        "dfweekday['weekend'] = mean_weekday\n",
        "dfweekday['mean_'] = round(dfweekday['count']/7,0)\n",
        "dfweekday.drop('weekend',axis=1,inplace=True)\n",
        "dfweekday"
      ],
      "metadata": {
        "id": "FHZWqHVF-GT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekd = [31866,31485]\n",
        "weekn = [31167,29968,28913,27846,26730] \n",
        "weekd_mean = np.mean(weekd)\n",
        "weekn_mean = np.mean(weekn)\n",
        "\n",
        "print(f'mean of week days trips that is m0: {weekn_mean}')\n",
        "print(f'mean of weekend days trips that is m1: {weekd_mean}')"
      ],
      "metadata": {
        "id": "WGxM4MCxJV6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 2 Conclusion: Number of trips on weekends are comparatively more than number of trips on weekdays.**"
      ],
      "metadata": {
        "id": "3bykSN-zN1e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Hypothesis 3: Traffic hours on weekdays are different than traffic hours on weekends."
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftraffic = df[['pickup_weekday','pickup_hour']]\n",
        "weekd = []\n",
        "for i in dftraffic['pickup_weekday']:\n",
        "  if i in ['Saturday','Sunday']:\n",
        "    weekd.append('Weekend')\n",
        "  else:\n",
        "    weekd.append('Weekday')\n",
        "dftraffic['weekd'] = weekd\n",
        "dftraffic.head(3)"
      ],
      "metadata": {
        "id": "7-xmGmLlPRFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new = pd.crosstab(dftraffic.pickup_hour,dftraffic.weekd)\n",
        "new = pd.DataFrame(new)\n",
        "new.reset_index(inplace=True)\n",
        "new['Total'] = new['Weekday'] + new['Weekend']\n",
        "new = new.iloc[0:24,:]\n",
        "new.set_index('pickup_hour',inplace=True)\n",
        "new.loc['Total'] = new.iloc[:, :].sum()\n",
        "new"
      ],
      "metadata": {
        "id": "C_nnSl2eSZqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As we are analysing two categorical variables, here We are going to use chi-squre test of independance."
      ],
      "metadata": {
        "id": "bpsNURDakFEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Calcualtion of Chisquare\n",
        "chi_square = 0\n",
        "rows = new.index.unique()\n",
        "columns = new.columns.unique()\n",
        "for i in columns:\n",
        "    for j in rows:\n",
        "        O = new[i][j]\n",
        "        E = new[i]['Total'] * new['Total'][j] / new['Total']['Total']\n",
        "        chi_square += (O-E)**2/E"
      ],
      "metadata": {
        "id": "0Ps4b1cYj70R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The p-value approach\n",
        "print(\"Approach 1: The p-value approach to hypothesis testing in the decision rule\")\n",
        "p_value = 1 - stats.chi2.cdf(chi_square, (len(rows)-1)*(len(columns)-1))\n",
        "conclusion = \"Failed to reject the null hypothesis.\"\n",
        "if p_value <= alpha:\n",
        "    conclusion = \"Null Hypothesis is rejected.\"\n",
        "        \n",
        "print(\"chisquare-score is:\", chi_square, \" and p value is:\", p_value)\n",
        "print(conclusion)"
      ],
      "metadata": {
        "id": "rmDsAa3-kV07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 3 Conclusion: Traffic hours on weekdays are different than traffic hours on weekends.**"
      ],
      "metadata": {
        "id": "3x1jweTbkmiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True) #Still confirming it by dropping them "
      ],
      "metadata": {
        "id": "-lSoxzEOdHCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'before: {df[df.duplicated()].shape}') # There are around 6 duplicate values in the dataset dropping them \n",
        "df.drop_duplicates(keep=\"first\", inplace=True)\n",
        "print(f'after: {df[df.duplicated()].shape}')"
      ],
      "metadata": {
        "id": "LJ0M_HoedR8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####There is no any missing value present in the dataset."
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####**Let's check below variables for any outliers present in them by, visualising boxplot.**"
      ],
      "metadata": {
        "id": "4RmXiXXNKc9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def outlier_per(df,x):\n",
        "\n",
        "  '''this function gives number of outliers present in the feature by using IQR method'''\n",
        "\n",
        "  q1 = df[x].quantile(0.25)\n",
        "  q3 = df[x].quantile(0.75)\n",
        "  iqr = q3-q1\n",
        "  lw = q1 - (1.5*iqr)\n",
        "  uw = q3 + (1.5*iqr)\n",
        "\n",
        "  cnt = 0\n",
        "  for i in df[x]:\n",
        "    if (i<lw)| (i>uw):\n",
        "      cnt+=1\n",
        "\n",
        "  strng = f'{round(cnt*100/len(df),2)} % Outliers present in column {x} in number it is {cnt}'\n",
        "  strng2 = f'mean: {round(df[x].mean(),2)}'\n",
        "  strng3 = f'median: {round(df[x].median(),2)}'\n",
        "  strng4 = f'minimum: {round(df[x].min(),2)}'\n",
        "  strng5 = f'maximum: {round(df[x].max(),2)}'\n",
        "  strng6 = f'kurtosis: {round(df[x].kurt(),2)}'\n",
        "  strng7 = f'skewness: {round(df[x].skew(),2)}'\n",
        "  \n",
        "  return(strng,strng2,strng3,strng4,strng5,strng6,strng7)"
      ],
      "metadata": {
        "id": "rj1z1SqgNxnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfcont = ['trip_duration','distance','speed']"
      ],
      "metadata": {
        "id": "_oV_kmWDUDkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfoutlier = df[dfcont]\n",
        "dfoutlier.head(3)"
      ],
      "metadata": {
        "id": "qcghS3ydvLfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outlier_replace(df,x):\n",
        "\n",
        "  '''this function replaces outliers lower than lower whisker with lower whicker and outliers higher than upper whisker with upper whicker'''\n",
        "\n",
        "  lw,uw = outlier_detection(df,x)\n",
        "  lstn = []\n",
        "  for i in df[x]:\n",
        "    if (i<lw):\n",
        "      lstn.append(lw)\n",
        "    if (i>uw):\n",
        "      lstn.append(uw)\n",
        "    else:\n",
        "      lstn.append(i)\n",
        "\n",
        "  df[x+'_out_replace'] = lstn"
      ],
      "metadata": {
        "id": "50ngdtt_wZ6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def outlier_detection(df,x):\n",
        "\n",
        "  '''this function gives lower whicker and upper whisker of the feature by using IQR method'''\n",
        "\n",
        "  q1 = df[x].quantile(0.25)\n",
        "  q3 = df[x].quantile(0.75)\n",
        "  iqr = q3-q1\n",
        "  lw = q1 - (1.5*iqr)\n",
        "  uw = q3 + (1.5*iqr)\n",
        "  return(lw,uw)"
      ],
      "metadata": {
        "id": "09IpFJj6zMSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dfcont:\n",
        "  outlier_replace(df=dfoutlier,x=i)"
      ],
      "metadata": {
        "id": "KZ3iMBYNyr7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfoutlier.head(3)"
      ],
      "metadata": {
        "id": "A0vHnxFh0lve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def boxplot_out(df,X,Y):\n",
        "\n",
        "    '''this function gives boxplot of variables with and without outliers'''\n",
        "\n",
        "    ## detecting outliers and calculating there %\n",
        "\n",
        "    strng0 = f'Boxplot of {X}'\n",
        "    strng = outlier_per(df,X)[0]\n",
        "    strng2 = outlier_per(df,X)[1]\n",
        "    strng3 = outlier_per(df,X)[2]\n",
        "    strng4 = outlier_per(df,X)[3]\n",
        "    strng5 = outlier_per(df,X)[4]\n",
        "    strng6 = outlier_per(df,X)[5]\n",
        "    strng7 = outlier_per(df,X)[6]\n",
        "    \n",
        "\n",
        "    ## plotting figure\n",
        "    fig, axes = plt.subplots(1,2, figsize=(22,6))\n",
        "    sns.boxplot(ax=axes[0], data=df,x=X)\n",
        "\n",
        "    ##labels\n",
        "\n",
        "    fig.text(0.35, 0.9,strng0,fontsize = 16,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.75,strng,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.70,strng2,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.65,strng3,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.60,strng4,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.55,strng5,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.50,strng6,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.45, 0.45,strng7,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "\n",
        "    strng0 = f'Boxplot of {Y}'\n",
        "    strng = outlier_per(df,Y)[0]\n",
        "    strng2 = outlier_per(df,Y)[1]\n",
        "    strng3 = outlier_per(df,Y)[2]\n",
        "    strng4 = outlier_per(df,Y)[3]\n",
        "    strng5 = outlier_per(df,Y)[4]\n",
        "    strng6 = outlier_per(df,Y)[5]\n",
        "    strng7 = outlier_per(df,Y)[6]\n",
        "    \n",
        "\n",
        "    ## plotting figure\n",
        "    # fig, axes = plt.subplots(1,2, figsize=(22,6))\n",
        "    sns.boxplot(ax=axes[1], data=df,x=Y)\n",
        "\n",
        "    ##labels\n",
        "    # plt.title(f'Boxplot of {Y}',size=14)\n",
        "\n",
        "    fig.text(0.80, 0.9,strng0,fontsize = 16,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.75,strng,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.70,strng2,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.65,strng3,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.60,strng4,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.55,strng5,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.50,strng6,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    fig.text(0.85, 0.45,strng7,fontsize = 12,color ='black',ha ='right',va ='bottom',alpha = 1.0)\n",
        "    \n",
        "    # show plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mHGAXvWufvTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boxplot_out(df=dfoutlier,X='trip_duration',Y='trip_duration_out_replace')"
      ],
      "metadata": {
        "id": "y05bpYFJlYgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boxplot_out(df=dfoutlier,X='distance',Y='distance_out_replace')"
      ],
      "metadata": {
        "id": "1RdzbmXCltoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####In above dataframe we are going to do categorical encoding for two columns that is 'Store_&_fwd_flag' and 'Pickup_weekday' as store and forward flag has only two variale we will be using Label encoding for it and for pickup weekday columns as there are 7 ordinal catgeorical variables we will be using replace method for this column as from monday to sunday in the range from 1 to 7 as Label encoder will asign numbers to respective days as per it's own way but we actually want to assign monday to sunday in ordinal way from 1 to 7."
      ],
      "metadata": {
        "id": "iP2V2nhsi_AW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "df['store_and_fwd_flag'] = le.fit_transform(df['store_and_fwd_flag']) \n",
        "df['pickup_weekday'] = df['pickup_weekday'].replace({'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6,'Sunday':7})\n",
        "df.head(3)"
      ],
      "metadata": {
        "id": "0AIyQVfUi-VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. From above columns we have already created distance column by using pickup and dropoff lattitudes and longitudes.\n",
        "2. We have created pickup weekday and pickuup hour from date time columns.\n",
        "3. So we are not going to use above columns as they are parent columns for above two columns which are being created by manupulations."
      ],
      "metadata": {
        "id": "UKERaIJAr6ZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We are selecting features on the basis of information which they are providing and removing features from dataset which are providing same kind of information. For that we have did correlation check.\n",
        "2. We have performed hypothesis testing to check weather features are relevant or not and from that we have found that pickup weekday and pickup hours are affecting number of trips and traffic times hence they are indirectly affecting trip duration.\n",
        "3. We have created few features by data manipulations such as distance."
      ],
      "metadata": {
        "id": "wjWXABCfv6s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfd = df.copy()\n",
        "dfd = dfd[['vendor_id','passenger_count','store_and_fwd_flag','trip_duration','distance','pickup_weekday','pickup_hour']]\n",
        "dfd.head(3)"
      ],
      "metadata": {
        "id": "JTzzzeL0rVhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I don't think there is any need of data transformation as we have already removed outliers from our dataset by replacing them with lower or upper whisker values and most of the columns are of categorical nature and we have already transformed them in desired format."
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We are not using satndard scaler to scale our data and bring our data points closer to each other as in our dataset we have only one independant variable which is contineous and all other varaibles are of categorical nature, that's we are not scaling data in this case.\n",
        "- Another reason to do so is we are not going to implement distance based algorithms in this case, so it is not necesary to bring our data points close to each other."
      ],
      "metadata": {
        "id": "GWCO9SeYlk7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Mulicollenearity check 2 (VIF Method)**\n",
        "- VIF stands for varaiance inflation factor\n",
        "- In VIF we regress every independant variable with each other and find the r-square\n",
        "- After finding r-square we use below VIF formula to find VIF index\n",
        "VIF index = 1/1-r.square\n",
        "- If VIF is more than 5 then we say that multi-collinearity exists"
      ],
      "metadata": {
        "id": "A83BCMNbgkjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vif(ind_var):\n",
        "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "    result_df = pd.DataFrame() \n",
        "    result_df[\"Feature\"] = ind_var.columns\n",
        "    result_df[\"vif\"] = [variance_inflation_factor(ind_var.values,i)for i in range(ind_var.shape[1])]\n",
        "    return result_df"
      ],
      "metadata": {
        "id": "5LpRF7tZhFMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vif(dfd.drop('trip_duration',axis=1))"
      ],
      "metadata": {
        "id": "ZQGFhV1RhTwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- No such multicollinearity is present in independant variables as VIF is less than 5 for almost all variables. So we are not going to perform Priniciple component analysis as our independant variable are only 6 in number and consists around 14,00,000 records so there is no any need of dimensionallity reduction in this case."
      ],
      "metadata": {
        "id": "6KnwQ1TIkdX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. We have left with only 6 independant features in our dataset after doing feature selection, feature modelling and feature engineering.\n",
        "#### 2. We have 14,00,000 records stored in 7 columns, hence no any problem will be there due curse of dimentionallity.\n",
        "#### 3. We have already reduced our features enough, without loosing originality of data."
      ],
      "metadata": {
        "id": "40GCIGOUrZaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfd.shape"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Let's split data into dependant and independant variables first."
      ],
      "metadata": {
        "id": "tBwHVOaZu9L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = dfd.drop('trip_duration',axis=1)\n",
        "y = dfd[['trip_duration']]"
      ],
      "metadata": {
        "id": "VGhi8rjXuP3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "dfd.to_csv('output.csv', encoding = 'utf-8-sig') \n",
        "files.download('output.csv')"
      ],
      "metadata": {
        "id": "36Cs_q04wOev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Now let's split data in the ratio of **80:20** where 80 % will be in training set and 20 % will be in testing set by using train_test_split function available in sklearn library."
      ],
      "metadata": {
        "id": "Lsbcpv-kvxs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=40)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "hdfiuuxfviZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Now let's split our test data in the ratio of **50:50** where 50 % will be in testing set and 50 % will be in live testing set that is validation set, that we will keep aside for future reference if we will be in the position to test our deployed model on live data."
      ],
      "metadata": {
        "id": "WVuLnyY9xkPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test,X_live,y_test,y_live = train_test_split(X_test,y_test,test_size=0.5,random_state=40)\n",
        "print(X_test.shape)\n",
        "print(X_live.shape)\n",
        "print(y_test.shape)\n",
        "print(y_live.shape)"
      ],
      "metadata": {
        "id": "biRYEsfryNh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### We have initially splitted data in 80/20 ratio as training and testing and again we have splitted testing data into two parts 50 % each one as test set and another as a validation set. {Train:Test:Validation = 80:10:10}"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Model Selection:**\n",
        "\n",
        "\n",
        "1.   Linear/Lasso/Ridge/Elastic Net: Linear regression has few assumptions; i) Linearity: when it comes to linearity not all the independant features are linearly affecting our dependant feature. ii) Normal Distribution: distribution of data points is not noraml, data is heavily skewed. So basiaclly our data is not following above two important assumptions that's why we will not considering above 4 ML models from implementation point of view.\n",
        "2.   Distance based algorithms/ Support vector machines: We have huge training dataset which consists around 10,00,000 records so if we use KNN then it will have high runtime as it will scan through all those records at the time of prediction so KNN is not an option in this case, same thing is there for support vectors as it will get so much time for hyperparameter tunning and it will also slow operation with such huge dataset, so that's why we are not considering those algorithms in this case.\n",
        "3. Decision tress: As we are not dealing with such a problem statment where inpretability/explainibility matters a lot, here we are focusing mostly on accuracy of predictions and we are expecting our working model to be as accarate as possible. So that' why we will consider weak learner based ensamble models or bootstrap aggrigation based random forest alogorithm in this case in this models we will have less explainibilty but we could get better results as compared to pure decision trees.\n",
        "4. So in this project we will be implementing Adaboost,Gboost,XGboost this three models independantly to check performance sequential ensembling methods and we will also implement Random forest regressor to check performance of bootstrap aggrigation methods, so after seeing results we will consider, any one model out of this four.\n"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**1. Adaboost Regressor**"
      ],
      "metadata": {
        "id": "_R1bbRM7n01y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## model training and intial performance\n",
        "\n",
        "adb = AdaBoostRegressor()\n",
        "adb.fit(X_train,y_train)\n",
        "y_pred = adb.predict(X_test)\n",
        "error = mean_squared_error(y_test,y_pred)\n",
        "rmse = np.sqrt(error)\n",
        "er_per = round((rmse*100/np.mean(np.array(y_test))),2)\n",
        "print(f'{er_per} %')"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model performance cross validation \n",
        "\n",
        "\n",
        "score_adb = cross_val_score(adb,X,y,cv=5)\n",
        "sc_adb = round(score_adb.mean(),2)\n",
        "print(f'{sc_adb * 100} %')"
      ],
      "metadata": {
        "id": "dWymq_DivjtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## hyperparameter tunning \n",
        "\n",
        "param_adb = {\"n_estimators\" : [10,30,50,70,90,100,150,200,400],\n",
        "         \"learning_rate\" : [0.0001,0.001,0.01,0.1,0.5,1.0]}\n",
        "\n",
        "tunned_adb=GridSearchCV(AdaBoostRegressor(),param_adb,cv=5)\n",
        "tunned_adb.fit(X,y)\n",
        "tunned_adb.best_params_ "
      ],
      "metadata": {
        "id": "PlEVancCvM-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## tunned model performance cross validation\n",
        "\n",
        "score_adb = cross_val_score(AdaBoostRegressor(learning_rate= 1.0, n_estimators= 60),X,y,cv=5)\n",
        "sc_adb = round(score_adb.mean(),2)\n",
        "print(f'{sc_adb} %')"
      ],
      "metadata": {
        "id": "Jrn47L182l5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**2. Gradient Boosting Regressor**"
      ],
      "metadata": {
        "id": "U6le4UoJ4DdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## model training and intial performance\n",
        "\n",
        "gb = GradientBoostingRegressor()\n",
        "gb.fit(X_train,y_train)\n",
        "y_pred = gb.predict(X_test)\n",
        "acc = r2_score(y_test,y_pred)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "1EQQr39z-h7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model performance cross validation\n",
        "\n",
        "score_gb = cross_val_score(gb,X,y,cv=5)\n",
        "sc_gb = round(score_gb.mean(),2)\n",
        "print(f'{sc_gb} %')"
      ],
      "metadata": {
        "id": "LctDMLoC4f62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## hyperparameter tunning \n",
        "param_gb = {\"alpha\": [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\"learning_rate\" : [0.001,0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]}\n",
        "tunned_gb=GridSearchCV(GradientBoostingRegressor(),param_gb,cv=5)\n",
        "tunned_gb.fit(X,y)\n",
        "tunned_gb.best_params_"
      ],
      "metadata": {
        "id": "H0mUWucI458A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## tunned model performance cross validation\n",
        "\n",
        "score_gb = cross_val_score(GradientBoostingRegressor(alpha= 0.6, learning_rate= 0.3),X,y,cv=5)\n",
        "sc_gb = round(score_gb.mean(),2)\n",
        "print(f'{sc_gb} %')"
      ],
      "metadata": {
        "id": "fLzqnPSE46SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**3. XG Boost Regressor**"
      ],
      "metadata": {
        "id": "eQDd1Gdg6MCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## model training and intial performance\n",
        "\n",
        "xgb = XGBRegressor()\n",
        "xgb.fit(X_train,y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "acc = r2_score(y_test,y_pred)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "jGReYmRf_XXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model performance cross validation\n",
        "\n",
        "score_xgb = cross_val_score(xgb,X,y,cv=5)\n",
        "sc_xgb = round(score_xgb.mean(),2)\n",
        "print(f'{sc_xgb} %')"
      ],
      "metadata": {
        "id": "vuWzY7wg6daG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## hyperparameter tunning \n",
        "param_xgb =  {'learning_rate' : [0.001,0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
        "         'reg_lambda' : [1,2,3,4,5,6,7,8,9,10],\n",
        "         'max_depth': [1,2,3,4,5,6,7,8,9,10]}\n",
        "tunned_xgb=GridSearchCV(XGBRegressor(),param_xgb,cv=5)\n",
        "tunned_xgb.fit(X,y)\n",
        "tunned_xgb.best_params_"
      ],
      "metadata": {
        "id": "i1s5qJT36vYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## tunned model performance cross validation\n",
        "\n",
        "score_xgb = cross_val_score(XGBRegressor(learning_rate= 0.1, max_depth= 10, reg_lambda= 7),X,y,cv=5)\n",
        "sc_xgb = round(score_xgb.mean(),2)\n",
        "print(f'{sc_xgb} %')"
      ],
      "metadata": {
        "id": "tK535J8a7xqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**4. Random Forest Regressor**"
      ],
      "metadata": {
        "id": "HpNNCn5C8iBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## model training and intial performance\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=50)\n",
        "rf.fit(X_train,y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "acc = r2_score(y_test,y_pred)\n",
        "print(acc) "
      ],
      "metadata": {
        "id": "kE9vilnQ_1EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model performance cross validation\n",
        "\n",
        "score_rf = cross_val_score(rf,X,y,cv=5)\n",
        "sc_rf = round(score_rf.mean(),2)\n",
        "print(f'{sc_rf} %')"
      ],
      "metadata": {
        "id": "OGq0x_8r82lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## hyperparameter tunning \n",
        "param_rf =  {\"n_estimators\":[10,20,30,40,50,60,70,80,90,110,130,150,170,190],'min_impurity_decrease':[0.0,0.5,1],\n",
        "               \"bootstrap\":[True,False]}\n",
        "tunned_rf=GridSearchCV(RandomForestRegressor(),param_rf,cv=5)\n",
        "tunned_rf.fit(X,y)\n",
        "tunned_rf.best_params_ "
      ],
      "metadata": {
        "id": "Tvc10m8R9HVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## tunned model performance cross validation\n",
        "\n",
        "score_rf = cross_val_score(RandomForestRegressor(bootstrap= True, min_impurity_decrease= 0.0, n_estimators= 150),X,y,cv=5)\n",
        "sc_rf = round(score_rf.mean(),2)\n",
        "print(f'{sc_rf} %')"
      ],
      "metadata": {
        "id": "l7MOcS979qgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}